# LLM 模型能力比較工具 - 開發指南

## 專案概述

### 目的
- 比較本地端 Ollama LLM 模型在繁體中文處理方面的能力
- 支援翻譯（英翻中）和歸納總結兩種任務
- 使用雲端大型語言模型（GPT、Gemini、DeepSeek）作為評審員
- 自動產生詳細的 Markdown 格式比較報告

### 技術棧
- **語言**: Python 3.8+
- **主要依賴**: ollama, openai, google-generativeai, requests
- **本地模型**: Ollama (支援 gemma, deepseek-coder, qwen 等)
- **雲端評審**: OpenAI GPT 系列, Google Gemini 系列, DeepSeek 系列
- **輸出格式**: Markdown 報告

## 專案結構

### 核心檔案
- `main.py` - 主程式入口點，處理命令列參數和工作流程
- `ollama_client.py` - Ollama 本地模型客戶端（目前為 Mock 實作）
- `reviewer_client.py` - 雲端評審模型客戶端（OpenAI、Gemini、DeepSeek）
- `reporter.py` - 報告生成器，產生 Markdown 格式的比較報告
- `config.py` - 配置檔案（不在版本控制中，包含 API 金鑰）

### 測試和範例檔案
- `sample_input.txt` - 範例輸入文本
- `integration_test_*.md` - 整合測試報告範例
- `test_*.md` - 單元測試報告範例

### 配置和文檔
- `README.md` - 完整的使用說明（繁體中文）
- `.gitignore` - 版本控制忽略檔案（包含 config.py）

## 開發最佳實踐

### 程式碼結構
- 使用類別封裝不同的客戶端（OllamaClient、各種 ReviewerClient）
- 統一的錯誤處理和日誌記錄
- 模組化設計，每個檔案負責特定功能
- 支援 Mock 模式進行測試

### 配置管理
- 所有敏感資訊（API 金鑰）存放在 `config.py` 中
- `config.py` 已加入 `.gitignore`，避免意外提交敏感資訊
- 使用預留位置（如 "YOUR_OPENAI_API_KEY"）來標示未配置的 API 金鑰
- 支援動態檢查 API 金鑰有效性

### 錯誤處理
- 使用 Python logging 模組進行統一日誌記錄
- 優雅處理 API 錯誤和網路問題
- 在報告中清楚標示錯誤狀態
- 支援部分失敗的情況（某些模型失敗不影響其他模型）

### 國際化和本地化
- 主要介面和報告使用繁體中文
- 程式碼註解和變數名使用英文
- 支援繁體中文文本處理和評估

## 開發工作流程

### 設定開發環境
1. 安裝 Python 3.8+
2. 安裝依賴套件：`pip install ollama openai google-generativeai requests`
3. 或使用 uv：`uv pip install ollama openai google-generativeai requests`
4. 安裝並運行 Ollama
5. 創建 `config.py` 檔案並配置 API 金鑰

### 配置檔案結構
```python
# config.py 範例結構
OLLAMA_API_BASE_URL = "http://localhost:11434"
OLLAMA_MODELS_TO_COMPARE = ["gemma:2b", "qwen:4b"]
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"
GOOGLE_API_KEY = "YOUR_GOOGLE_API_KEY"
DEEPSEEK_API_KEY = "YOUR_DEEPSEEK_API_KEY"
REVIEWER_MODELS = {
    "gpt": "gpt-3.5-turbo",
    "gemini": "gemini-pro",
    "deepseek": "deepseek-chat"
}
SUPPORTED_TASKS = {
    "translate": "Translate to Traditional Chinese",
    "summarize": "Summarize text"
}
```

### 執行測試
- 使用 `sample_input.txt` 進行基本測試
- 執行翻譯任務：`python main.py --input_file sample_input.txt --task translate`
- 執行總結任務：`python main.py --input_file sample_input.txt --task summarize`

### 新增功能
- 新增評審模型：在 `reviewer_client.py` 中創建新的客戶端類別
- 新增任務類型：更新 `SUPPORTED_TASKS` 配置和相關邏輯
- 修改報告格式：編輯 `reporter.py` 中的報告生成邏輯

## 安全注意事項

### API 金鑰管理
- 絕不將 `config.py` 提交到版本控制系統
- 使用環境變數或安全的金鑰管理系統
- 定期輪換 API 金鑰
- 監控 API 使用量和費用

### 資料隱私
- 注意輸入文本可能包含敏感資訊
- 了解各雲端服務的資料處理政策
- 考慮使用本地模型進行敏感資料處理

## 故障排除

### 常見問題
- **ImportError: No module named 'config'**: 確保 `config.py` 檔案存在且格式正確
- **API 金鑰錯誤**: 檢查 `config.py` 中的 API 金鑰是否有效
- **Ollama 連接失敗**: 確保 Ollama 服務正在運行且可訪問
- **模型不存在**: 確保 Ollama 中已下載指定的模型

### 除錯技巧
- 檢查日誌輸出以了解執行狀態
- 使用 Mock 模式進行測試
- 逐步測試各個組件
- 檢查網路連接和防火牆設定

## 擴展指南

### 新增評審模型
1. 在 `reviewer_client.py` 中創建新的客戶端類別
2. 繼承 `BaseReviewerClient`
3. 實作 `evaluate` 方法
4. 更新 `config.py` 中的 `REVIEWER_MODELS`
5. 在 `main.py` 中新增初始化邏輯

### 新增任務類型
1. 更新 `config.py` 中的 `SUPPORTED_TASKS`
2. 修改 `ollama_client.py` 中的任務處理邏輯
3. 更新 `reporter.py` 中的評分維度
4. 新增相應的測試案例

### 改進報告格式
- 修改 `reporter.py` 中的 `generate_report` 函數
- 新增圖表和視覺化元素
- 支援多種輸出格式（HTML、PDF 等）
- 新增統計分析功能

## 開發規則

### 基本規則
1. **語言使用規則**: 所有回覆和溝通必須使用繁體中文 (zh-TW)
2. **程式碼註解規則**: 寫程式時必須加上詳細的繁體中文註解，包括：
   - 變數宣告的用途說明
   - 函數宣告的功能描述、參數說明、回傳值說明
   - 程式區塊的邏輯說明和執行流程

### 程式碼註解範例

#### 變數宣告註解
```python
# 儲存所有 Ollama 模型的比較結果
all_results = []

# 用於儲存當前處理的模型結果資料
model_result = {
    "ollama_model": ollama_model_name,  # 當前測試的 Ollama 模型名稱
    "task": args.task,                  # 執行的任務類型（翻譯或總結）
    "input_text_snippet": input_snippet_for_report,  # 輸入文本片段
    "ollama_output": "<not_run>",       # Ollama 模型的輸出結果
    "reviews": [],                      # 評審結果列表
}

# API 金鑰配置變數
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"  # OpenAI API 金鑰
GOOGLE_API_KEY = "YOUR_GOOGLE_API_KEY"  # Google Gemini API 金鑰
```

#### 函數宣告註解
```python
def load_input_text(file_path: str) -> str:
    """
    載入輸入文本檔案
    
    參數:
        file_path (str): 輸入檔案的路徑
    
    回傳:
        str: 檔案內容文本
    
    異常:
        Exception: 當檔案讀取失敗時拋出異常
    """
    try:
        # 使用 UTF-8 編碼開啟檔案
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        # 記錄錯誤訊息並重新拋出異常
        logging.error(f"讀取檔案 {file_path} 時發生錯誤: {e}")
        raise

def generate_report(all_results, output_filepath: str):
    """
    生成 Markdown 格式的比較報告
    
    參數:
        all_results: 包含所有模型比較結果的列表
        output_filepath (str): 報告輸出檔案路徑
    
    功能:
        - 解析模型比較結果
        - 格式化為 Markdown 格式
        - 儲存到指定檔案路徑
    """
    # 初始化報告內容列表
    report_lines = []
    
    # 添加報告標題
    report_lines.append("# LLM 模型比較報告")
```

#### 程式區塊註解
```python
# === 初始化評審客戶端區塊 ===
reviewers = []  # 儲存所有評審客戶端的列表

# 檢查並初始化 OpenAI 評審客戶端
if REVIEWER_MODELS.get("gpt"):
    reviewers.append(
        OpenAIReviewerClient(
            api_key=OPENAI_API_KEY, 
            model_name=REVIEWER_MODELS["gpt"]
        )
    )

# 檢查並初始化 Gemini 評審客戶端
if REVIEWER_MODELS.get("gemini"):
    reviewers.append(
        GeminiReviewerClient(
            api_key=GOOGLE_API_KEY, 
            model_name=REVIEWER_MODELS["gemini"]
        )
    )

# === 模型處理主迴圈區塊 ===
for ollama_model_name in OLLAMA_MODELS_TO_COMPARE:
    logging.info(f"--- 開始處理 Ollama 模型: {ollama_model_name} ---")
    
    # 準備輸入文本片段用於報告顯示
    input_snippet_for_report = (
        input_text[:200] + "..." if len(input_text) > 200 else input_text
    )
    
    # 嘗試執行 Ollama 模型生成
    try:
        # 呼叫 Ollama 客戶端生成回應
        ollama_output = ollama_client.generate(
            ollama_model_name, input_text, args.task
        )
        model_result["ollama_output"] = ollama_output
        
        # === 評審處理子區塊 ===
        # 對每個活躍的評審模型進行評估
        for reviewer in active_reviewers:
            logging.info(f"使用評審模型 {reviewer.model_name} 進行評估...")
            
            try:
                # 執行評審並收集結果
                review_data = reviewer.evaluate(
                    input_text, ollama_output, args.task
                )
                # 將評審結果添加到模型結果中
                model_result["reviews"].append({
                    "reviewer_model": reviewer.model_name,
                    "evaluation": review_data,
                })
            except Exception as e:
                # 記錄評審過程中的錯誤
                logging.error(f"評審模型 {reviewer.model_name} 發生錯誤: {e}")
                
    except Exception as e:
        # 處理 Ollama 模型執行錯誤
        model_result["ollama_output"] = {"error": str(e)}
        logging.error(f"Ollama 模型 {ollama_model_name} 執行錯誤: {e}")
```

## 貢獻指南

### 程式碼風格
- 使用 Python PEP 8 風格指南
- 函數和變數名使用英文
- 所有註解和文檔必須使用繁體中文
- 保持程式碼簡潔和可讀性
- 每個函數、類別、重要變數都必須有繁體中文註解

### 提交規範
- 使用清楚的提交訊息
- 每個提交專注於單一功能或修復
- 包含適當的測試
- 更新相關文檔

### 測試要求
- 新功能必須包含測試
- 確保現有測試通過
- 測試覆蓋率應保持在合理水平
- 包含整合測試和單元測試